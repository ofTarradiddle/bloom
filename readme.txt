R 
Notes and Conclusions

Q1
For Q1 it is important to exercise caution when incorporating primary id logic to ensure that the solution aligns with the prompt's expectations, i.e. the Tenor should not have a trailing 0, as demonstrated in the example. The prompt also indicates that where the InstrumentType is 'Index', the ClientEntity should be blank. In the following sentence, we are asked to create a new composite index of CMATicker-Tenor, but not exclusively for InstrumentType is 'Index'. It is noteworthy that where the InstrumentType is 'Tranche', the ClientEntityId is also blank, and the Sector is described as 'Index Tranche'. As such, it is reasonable to assume that this too should be overridden, as the vendor instructions perhaps also forgot to differentiate Index vs Index Tranche. 

Q2
In Q2, we are tasked with providing summary statistics by the Industry Sector Group of PE_RATIOS. We must first ensure that the values used are appropriate for numeric calculations. The PE_RATIO column, for instance, contains non-numeric values. By using pd.to_numeric and setting errors to 'coerce', we inform pandas that invalid parsings will be set to NaN. By storing the NaN indices and looking them up from a copy of the original data frame, we can identify the companies with no associated PE values and see, oddly, that a company with high coverage such as Google is missing their associate value.

When describing our PE ratios by sector, we can use the simple groupby.describe() method, However, we can also be more explicit with the grouped aggregations calculated by specifying custom functions. 

In our PE ratio plot, I choose to keep the outliers to capture visually the pe ratio sector min-max spread. 

Q3
In Q3, we are tasked with running linear regression and making arbitrary conclusions. This likely entails taking a position on the functional form of the relationship in the data and evaluating whether a linear model is well-specified given its assumptions. To summarize, linear regression refers to an affine model (Y = WX + b), with the squared error loss.

To find B (W in the formula), we need to set the derivative of our loss with respect to W or b to 0 and solve for the parameter of interest. Although OLS has a closed-form solution, we can also use iterative algorithms such as Newton's method.

The main assumptions in OLS are 1) expectations of epsilon (true error, not the residual) is zero, 2) variance of epsilon is constant and thus does not depend on the x's, and 3) no correlation between epsilon. When these assumptions hold, the least squares estimators of B's are well approximated and have nice properties. The coefficient solution B = (XtX)^-1(XtX) arises purely from solving the least squares optimization problem, and that solely fitting the linear model is not enough to ensure our estimate is BLUE (best linear unbiased estimator, which conatins further assumptions not specified here). That is, our model can be linear in the data and perform poorly if not well-specified.

After loading our data from the files class and running a correlation analysis, we discovered that two variables (1,3) are perfectly correlated, which violates the no multicollinearity assumption. This issue can be resolved by dropping one of the variables. OLS is scale and shift-invariant, as are regularized linear models like ridge and lasso, and the random forest model used in the next question. Hence, we do not need to worry about the difference in scale across the variables as our solution will be consistent with and without scale/shift preprocessing.

We can split our test/train set by specifying a splitInt value, where the indices up to splitInt are used for training, and the remaining indices are used for testing. Upon fitting our model, we observe a relatively high R^2, indicating that our model explains a significant portion of the variance observed in the training data. Exogeneity does not seem to be an issue as the out of sample correlation between the residual vector and the regressors are low. 

Our condition number is elevated, which may indicate some misspecification. To interpret how this affects our results, we must first understand what the condition number represents and how it is calculated. The condition number of a matrix A is defined as the ratio of the largest and smallest singular values of A. It measures the sensitivity of the output of a function or model to changes in the input data. In the context of linear regression, the condition number is used to evaluate the numerical stability of the matrix inversion operation that is utilized to estimate the regression coefficients. Consequently, minor changes in the input data may cause significant variations in the output of the model, making it challenging to interpret the coefficients and generate accurate predictions. In practice, a high condition number can indicate multicollinearity, which is the presence of high correlations between the predictor variables. In our case, removing the insignificant variables (i.e., variables 1, 7, and 8) from the regression alleviates the large condition number while maintaining a high adjusted R*2 of 80%.

Our diagnostics plots suggest our model is in fact well specified from the fact that our residual vs fitted plot is relatively flat, the normal Q-Q plot lies mostly on the theoretical quantiles suggesting normal errors, the scale location plot suggests there may be some heteroskedasity (typical solutions for this involve weighted least squares, or robust standard error models), and there appear to be no influential data points as measured by cook's distances less than .5 (Cook's distance measures the influence of observations in our data matrix and their effect on coefficient estiamtes).  For a comprehensive review of diagnostic tests on linear regression, see "Linear Models in Statistics" by Brunner, Chapter 9, "Model Validation and Diagnostics". 

Last, but not least, our out of sample predictions seem directionally accurate to observed values, and see only a slight increas in mse relative to the train set. 

Q4
The Boston housing dataset is a widely-used dataset in many introductory machine learning courses. In Q4, we are tasked with using both a linear regression and random forest algorithm to predict medv (i.e., the median value of owner-occupied homes in $1000s) and evaluating the models by comparing their test MSE via 10-fold cross-validation.

Regarding our methodology choice of ten-fold cross-validation, we split the data into ten approximately equal-sized subsets, or "folds." The model is then trained on nine of the folds and evaluated on the remaining fold. This process is repeated ten times, with each fold serving as the test set once. The results from each fold are then averaged to obtain an estimate of the model's performance.

Upon evaluating the models, we observe that the test mean squared error of linear regression is nearly double that of random forest, suggesting that a nonlinear relationship may be the correct model form. Visualizing the target variable in histogram form allows us to gain a sense of the number and types of outliers, and we recognize that there is a noticeable portion of target values that lie at extreme values. We can filter out these outliers in the dataset by limiting observations to those within three standard deviations of the target variable. Alternatively, we could remove the outliers with a medv value of 50. With this filtration method, both models perform better, but random forest's marginal benefit remains about the same relative to our linear model.

The linear regression model may be at a disadvantage if not based solely on the implicit target shaping that has been performed during the curation of this dataset. Our objective is to predict the median values of homes that have been collected by the U.S. Census Service and, importantly, aggregated at the town level. 

In predicting, the Random Forest algorithm passes the input through each decision tree in the forest, and the prediction for the input is calculated at the leaf node of each decision tree by averaging the target variables. The final prediction for the input is obtained by averaging the prediction across all the decision trees. This explicit averaging ensures that the random forest model will not predict values outside of those seen in the training data. Moreover, the housing dataset seems to winsorize values at 50, further enhancing the similarities of random forest to this dataset. On the other hand, linear regression can generate extreme positive or negative values through its linear extrapolation of the learned hyperplane. Therefore, the random forest model may be a more appropriate choice for this dataset, as the aggregation methodology that forms the dataset is more closely aligned with the leaf node aggregating, and cross-tree averaging, of the random forest algorithm.

Q5 
In Q5 we are asked to complete two sql queries, paying attention to appropriate data types, and a mapping table. Both queries require us to filter on the Date var which is represented as an integer, as such we can pass a YYYYMMDD int to filter our dates. Additionally, we need to filter on the tenor_prices variable, which is defined as decimal(4,2), allowing for a maximum of 4 digits with 2 digits after the decimal point, which means that 5.00 can be used to filter this column appropriately. The question asks for the ticker values, but it does not specify which ticker value to return from the table as multiple exist. I assume that ticker refers to CMATicker as we know that CDS contracts originated in sovereign markets in the 1990s, hence the 'RUSSIA' filter, but that much of the volume has moved to corporate entities suggesting that the ticker column likely holds soverigns and corporations. Furthermore, CMA, a subsidiary of CME Group, being a provider of CDS data further adds credence to 'CMATicker' being the appropriate id. 
